{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "58f2b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "8a100a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(backbone, self).__init__()\n",
    "        model = torchvision.models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-3])\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.query_conv=nn.Conv2d(1024,256, 1)\n",
    "        self.target_conv=nn.Conv2d(1024,256, 3)\n",
    "        self.flatten = nn.Flatten(start_dim=2,end_dim=3)\n",
    "    def forward(self, query,target):\n",
    "        query_features = self.features(query)\n",
    "        query_features_conv=self.query_conv(query_features)\n",
    "        query_features_conv_flatten=self.flatten(query_features_conv)\n",
    "        query_features_conv_flatten=query_features_conv_flatten.permute(0, 2, 1)\n",
    "        \n",
    "        target_features = self.features(target)\n",
    "        target_features_conv=self.target_conv(target_features)\n",
    "        target_features_conv_flatten=self.flatten(target_features_conv)\n",
    "        target_features_conv_flatten=target_features_conv_flatten.permute(0, 2, 1)\n",
    "        return query_features_conv_flatten,target_features_conv_flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ed2961a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res50_model = torchvision.models.resnet50(pretrained=True)\n",
    "res50_ = backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "af25ee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "backbone                                      [1, 64, 256]              --\n",
       "├─Sequential: 1-1                             [1, 1024, 8, 8]           --\n",
       "│    └─Conv2d: 2-1                            [1, 64, 64, 64]           (9,408)\n",
       "│    └─BatchNorm2d: 2-2                       [1, 64, 64, 64]           (128)\n",
       "│    └─ReLU: 2-3                              [1, 64, 64, 64]           --\n",
       "│    └─MaxPool2d: 2-4                         [1, 64, 32, 32]           --\n",
       "│    └─Sequential: 2-5                        [1, 256, 32, 32]          --\n",
       "│    │    └─Bottleneck: 3-1                   [1, 256, 32, 32]          (75,008)\n",
       "│    │    └─Bottleneck: 3-2                   [1, 256, 32, 32]          (70,400)\n",
       "│    │    └─Bottleneck: 3-3                   [1, 256, 32, 32]          (70,400)\n",
       "│    └─Sequential: 2-6                        [1, 512, 16, 16]          --\n",
       "│    │    └─Bottleneck: 3-4                   [1, 512, 16, 16]          (379,392)\n",
       "│    │    └─Bottleneck: 3-5                   [1, 512, 16, 16]          (280,064)\n",
       "│    │    └─Bottleneck: 3-6                   [1, 512, 16, 16]          (280,064)\n",
       "│    │    └─Bottleneck: 3-7                   [1, 512, 16, 16]          (280,064)\n",
       "│    └─Sequential: 2-7                        [1, 1024, 8, 8]           --\n",
       "│    │    └─Bottleneck: 3-8                   [1, 1024, 8, 8]           (1,512,448)\n",
       "│    │    └─Bottleneck: 3-9                   [1, 1024, 8, 8]           (1,117,184)\n",
       "│    │    └─Bottleneck: 3-10                  [1, 1024, 8, 8]           (1,117,184)\n",
       "│    │    └─Bottleneck: 3-11                  [1, 1024, 8, 8]           (1,117,184)\n",
       "│    │    └─Bottleneck: 3-12                  [1, 1024, 8, 8]           (1,117,184)\n",
       "│    │    └─Bottleneck: 3-13                  [1, 1024, 8, 8]           (1,117,184)\n",
       "├─Conv2d: 1-2                                 [1, 256, 8, 8]            262,400\n",
       "├─Flatten: 1-3                                [1, 256, 64]              --\n",
       "├─Sequential: 1-4                             [1, 1024, 63, 38]         (recursive)\n",
       "│    └─Conv2d: 2-8                            [1, 64, 500, 300]         (recursive)\n",
       "│    └─BatchNorm2d: 2-9                       [1, 64, 500, 300]         (recursive)\n",
       "│    └─ReLU: 2-10                             [1, 64, 500, 300]         --\n",
       "│    └─MaxPool2d: 2-11                        [1, 64, 250, 150]         --\n",
       "│    └─Sequential: 2-12                       [1, 256, 250, 150]        (recursive)\n",
       "│    │    └─Bottleneck: 3-14                  [1, 256, 250, 150]        (recursive)\n",
       "│    │    └─Bottleneck: 3-15                  [1, 256, 250, 150]        (recursive)\n",
       "│    │    └─Bottleneck: 3-16                  [1, 256, 250, 150]        (recursive)\n",
       "│    └─Sequential: 2-13                       [1, 512, 125, 75]         (recursive)\n",
       "│    │    └─Bottleneck: 3-17                  [1, 512, 125, 75]         (recursive)\n",
       "│    │    └─Bottleneck: 3-18                  [1, 512, 125, 75]         (recursive)\n",
       "│    │    └─Bottleneck: 3-19                  [1, 512, 125, 75]         (recursive)\n",
       "│    │    └─Bottleneck: 3-20                  [1, 512, 125, 75]         (recursive)\n",
       "│    └─Sequential: 2-14                       [1, 1024, 63, 38]         (recursive)\n",
       "│    │    └─Bottleneck: 3-21                  [1, 1024, 63, 38]         (recursive)\n",
       "│    │    └─Bottleneck: 3-22                  [1, 1024, 63, 38]         (recursive)\n",
       "│    │    └─Bottleneck: 3-23                  [1, 1024, 63, 38]         (recursive)\n",
       "│    │    └─Bottleneck: 3-24                  [1, 1024, 63, 38]         (recursive)\n",
       "│    │    └─Bottleneck: 3-25                  [1, 1024, 63, 38]         (recursive)\n",
       "│    │    └─Bottleneck: 3-26                  [1, 1024, 63, 38]         (recursive)\n",
       "├─Conv2d: 1-5                                 [1, 256, 61, 36]          2,359,552\n",
       "├─Flatten: 1-6                                [1, 256, 2196]            --\n",
       "===============================================================================================\n",
       "Total params: 11,165,248\n",
       "Trainable params: 2,621,952\n",
       "Non-trainable params: 8,543,296\n",
       "Total mult-adds (G): 45.81\n",
       "===============================================================================================\n",
       "Input size (MB): 7.40\n",
       "Forward/backward pass size (MB): 59.42\n",
       "Params size (MB): 44.66\n",
       "Estimated Total Size (MB): 111.47\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n",
    "model = backbone().to(device)\n",
    "summary(model=res50_,\n",
    "         input_size=[(1,3,128,128),(1,3,1000,600)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "b758e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.rand(2,3,128,128)\n",
    "target = torch.rand(2,3,1000,600)\n",
    "query,target= res50_(query,target)\n",
    "# print(query.shape,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "ed10b64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 256])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query=query.reshape(64,256)\n",
    "# target=target.reshape(2196,256)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "69d4ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embedding_query = nn.Parameter(torch.rand(1,\n",
    "                                             query.shape[1], \n",
    "                                             query.shape[2]),\n",
    "                                  requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "8400c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embedding_target = nn.Parameter(torch.rand(1,\n",
    "                                             target.shape[1], \n",
    "                                             target.shape[2]),\n",
    "                                  requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "1058c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pe=query+position_embedding_query\n",
    "target_pe=target+position_embedding_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "5792e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=256,\n",
    "                 num_heads:int=8, # \n",
    "                 attn_dropout:int=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "        \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self,q,k,v):\n",
    "#         print (k.shape)\n",
    "        q = self.layer_norm(q)\n",
    "        k = self.layer_norm(k)\n",
    "        v = self.layer_norm(v)\n",
    "#         print (q.shape)\n",
    "        attn_output, _ = self.multihead_attn(query=q, # query embeddings \n",
    "                                             key=k, # key embeddings\n",
    "                                             value=v, # value embeddings\n",
    "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "3780e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape of MSA block: torch.Size([2, 64, 256])\n",
      "Output shape MSA block: torch.Size([2, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of MSABlock\n",
    "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=256, \n",
    "                                                             num_heads=8) \n",
    "\n",
    "# Pass patch and position image embedding through MSABlock\n",
    "patched_image_through_msa_block = multihead_self_attention_block(query_pe,target_pe,target)\n",
    "print(f\"Input shape of MSA block: {query.shape}\")\n",
    "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "6f06e9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MultiheadSelfAttentionBlock              [64, 256]                 --\n",
       "├─LayerNorm: 1-1                         [64, 256]                 512\n",
       "├─LayerNorm: 1-2                         [2196, 256]               (recursive)\n",
       "├─LayerNorm: 1-3                         [2196, 256]               (recursive)\n",
       "├─MultiheadAttention: 1-4                [64, 256]                 263,168\n",
       "==========================================================================================\n",
       "Total params: 263,680\n",
       "Trainable params: 263,680\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.28\n",
       "==========================================================================================\n",
       "Input size (MB): 4.56\n",
       "Forward/backward pass size (MB): 0.13\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 4.70\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=multihead_self_attention_block,\n",
    "         input_size=[(64, 256),(2196 ,256 ),(2196,256)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "d5dd30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=256, # Hidden Size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=1024, # MLP size from Table 1 for ViT-Base\n",
    "                 dropout:int=0.1): # Dropout from Table 3 for ViT-Base\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.ReLU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
    "                      out_features=embedding_dim), # take back to embedding_dim\n",
    "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "    \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "0b675a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape of MLP block: torch.Size([2, 64, 256])\n",
      "Output shape MLP block: torch.Size([2, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "mlp_block = MLPBlock(embedding_dim=256, # from Table 1 \n",
    "                     mlp_size=1024, # from Table 1\n",
    "                     dropout=0.1) # from Table 3\n",
    "\n",
    "# Pass output of MSABlock through MLPBlock\n",
    "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
    "print(f\"Input shape of MLP block: {patched_image_through_mlp_block.shape}\")\n",
    "print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "072a0d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MLPBlock                                 [64, 256]                 --\n",
       "├─LayerNorm: 1-1                         [64, 256]                 512\n",
       "├─Sequential: 1-2                        [64, 256]                 --\n",
       "│    └─Linear: 2-1                       [64, 1024]                263,168\n",
       "│    └─ReLU: 2-2                         [64, 1024]                --\n",
       "│    └─Dropout: 2-3                      [64, 1024]                --\n",
       "│    └─Linear: 2-4                       [64, 256]                 262,400\n",
       "│    └─Dropout: 2-5                      [64, 256]                 --\n",
       "==========================================================================================\n",
       "Total params: 526,080\n",
       "Trainable params: 526,080\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 33.67\n",
       "==========================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 0.79\n",
       "Params size (MB): 2.10\n",
       "Estimated Total Size (MB): 2.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=mlp_block,\n",
    "         input_size=[(64, 256)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "28807ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=256, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=8, # Heads from Table 1 for ViT-Base\n",
    "                 mlp_size:int=1024, # MLP size from Table 1 for ViT-Base\n",
    "                 mlp_dropout:int=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                 attn_dropout:int=0): # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "        \n",
    "    # 5. Create a forward() method  \n",
    "    def forward(self, q,k,v):\n",
    "        \n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        q =  self.msa_block(q,k,v) + q \n",
    "        \n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        q = self.mlp_block(q) + q\n",
    "        \n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6df76d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 256])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create an instance of TransformerEncoderB`lock\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "\n",
    "# # # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
    "summary(model=transformer_encoder_block,\n",
    "          input_size=[(64, 256),(2196 ,256 ),(2196,256)])\n",
    "transformer_encoder_block(query_pe,target_pe,target).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "5d640cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class one_shot(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 query_size:int=128,\n",
    "                 target_width:int=1000,\n",
    "                 target_height:int=600,\n",
    "                 in_channels:int=3, # Number of channels in input image# Patch size\n",
    "                 num_transformer_layers:int=4, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=256, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=1024, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=8, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:int=0, # Dropout for attention projection\n",
    "                 mlp_dropout:int=0.1, # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:int=0.1): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "        \n",
    "        self.backbone=backbone()\n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding_query = nn.Parameter(data=torch.randn(1,  math.ceil(query_size/16)*math.ceil(query_size/16), embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "        self.position_embedding_target = nn.Parameter(data=torch.randn(1, math.ceil(target_width/16-2)*math.ceil(target_height/16-2), embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "#         self.position_embedding_query = nn.Parameter(data=torch.randn(1,  64, embedding_dim),\n",
    "#                                                requires_grad=True)\n",
    "#         self.position_embedding_target = nn.Parameter(data=torch.randn(1, 2196, embedding_dim),\n",
    "#                                                requires_grad=True)\n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) \n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout)\n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, query,target):\n",
    "        \n",
    "        # 12. Get batch size\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        \n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        query,target= self.backbone(query,target)\n",
    "        \n",
    "\n",
    "        # 16. Add position embedding to patch embedding (equation 1) \n",
    "        query_pe = self.position_embedding_query + query\n",
    "        target_pe = self.position_embedding_target + target\n",
    "\n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        q=self.transformer_encoder(query_pe,target_pe,target)\n",
    "        t=self.transformer_encoder(target_pe,query_pe,query)\n",
    "        q_pe=self.position_embedding_query + q\n",
    "        t_pe=self.position_embedding_target + t\n",
    "        for i in range(3):\n",
    "            q=self.transformer_encoder(q_pe,t_pe,t)\n",
    "            t=self.transformer_encoder(t_pe,q_pe,q)    \n",
    "            q_pe=self.position_embedding_query + q\n",
    "            t_pe=self.position_embedding_target + t\n",
    "\n",
    "        return q,t  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "49bb6ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "one_shot                                           [1, 64, 256]              578,560\n",
       "├─backbone: 1-1                                    [1, 64, 256]              --\n",
       "│    └─Sequential: 2-1                             [1, 1024, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-1                            [1, 64, 64, 64]           (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       [1, 64, 64, 64]           (128)\n",
       "│    │    └─ReLU: 3-3                              [1, 64, 64, 64]           --\n",
       "│    │    └─MaxPool2d: 3-4                         [1, 64, 32, 32]           --\n",
       "│    │    └─Sequential: 3-5                        [1, 256, 32, 32]          (215,808)\n",
       "│    │    └─Sequential: 3-6                        [1, 512, 16, 16]          (1,219,584)\n",
       "│    │    └─Sequential: 3-7                        [1, 1024, 8, 8]           (7,098,368)\n",
       "│    └─Conv2d: 2-2                                 [1, 256, 8, 8]            262,400\n",
       "│    └─Flatten: 2-3                                [1, 256, 64]              --\n",
       "│    └─Sequential: 2-4                             [1, 1024, 63, 38]         (recursive)\n",
       "│    │    └─Conv2d: 3-8                            [1, 64, 500, 300]         (recursive)\n",
       "│    │    └─BatchNorm2d: 3-9                       [1, 64, 500, 300]         (recursive)\n",
       "│    │    └─ReLU: 3-10                             [1, 64, 500, 300]         --\n",
       "│    │    └─MaxPool2d: 3-11                        [1, 64, 250, 150]         --\n",
       "│    │    └─Sequential: 3-12                       [1, 256, 250, 150]        (recursive)\n",
       "│    │    └─Sequential: 3-13                       [1, 512, 125, 75]         (recursive)\n",
       "│    │    └─Sequential: 3-14                       [1, 1024, 63, 38]         (recursive)\n",
       "│    └─Conv2d: 2-5                                 [1, 256, 61, 36]          2,359,552\n",
       "│    └─Flatten: 2-6                                [1, 256, 2196]            --\n",
       "├─TransformerEncoderBlock: 1-2                     [1, 64, 256]              --\n",
       "│    └─MultiheadSelfAttentionBlock: 2-7            [1, 64, 256]              --\n",
       "│    │    └─LayerNorm: 3-15                        [1, 64, 256]              512\n",
       "│    │    └─LayerNorm: 3-16                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-17                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─MultiheadAttention: 3-18               [1, 64, 256]              263,168\n",
       "│    └─MLPBlock: 2-8                               [1, 64, 256]              --\n",
       "│    │    └─LayerNorm: 3-19                        [1, 64, 256]              512\n",
       "│    │    └─Sequential: 3-20                       [1, 64, 256]              525,568\n",
       "├─TransformerEncoderBlock: 1-3                     [1, 2196, 256]            (recursive)\n",
       "│    └─MultiheadSelfAttentionBlock: 2-9            [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-21                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-22                        [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-23                        [1, 64, 256]              (recursive)\n",
       "│    │    └─MultiheadAttention: 3-24               [1, 2196, 256]            (recursive)\n",
       "│    └─MLPBlock: 2-10                              [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-25                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─Sequential: 3-26                       [1, 2196, 256]            (recursive)\n",
       "├─TransformerEncoderBlock: 1-4                     [1, 64, 256]              (recursive)\n",
       "│    └─MultiheadSelfAttentionBlock: 2-11           [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-27                        [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-28                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-29                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─MultiheadAttention: 3-30               [1, 64, 256]              (recursive)\n",
       "│    └─MLPBlock: 2-12                              [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-31                        [1, 64, 256]              (recursive)\n",
       "│    │    └─Sequential: 3-32                       [1, 64, 256]              (recursive)\n",
       "├─TransformerEncoderBlock: 1-5                     [1, 2196, 256]            (recursive)\n",
       "│    └─MultiheadSelfAttentionBlock: 2-13           [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-33                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-34                        [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-35                        [1, 64, 256]              (recursive)\n",
       "│    │    └─MultiheadAttention: 3-36               [1, 2196, 256]            (recursive)\n",
       "│    └─MLPBlock: 2-14                              [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-37                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─Sequential: 3-38                       [1, 2196, 256]            (recursive)\n",
       "├─TransformerEncoderBlock: 1-6                     [1, 64, 256]              (recursive)\n",
       "│    └─MultiheadSelfAttentionBlock: 2-15           [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-39                        [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-40                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-41                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─MultiheadAttention: 3-42               [1, 64, 256]              (recursive)\n",
       "│    └─MLPBlock: 2-16                              [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-43                        [1, 64, 256]              (recursive)\n",
       "│    │    └─Sequential: 3-44                       [1, 64, 256]              (recursive)\n",
       "├─TransformerEncoderBlock: 1-7                     [1, 2196, 256]            (recursive)\n",
       "│    └─MultiheadSelfAttentionBlock: 2-17           [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-45                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-46                        [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-47                        [1, 64, 256]              (recursive)\n",
       "│    │    └─MultiheadAttention: 3-48               [1, 2196, 256]            (recursive)\n",
       "│    └─MLPBlock: 2-18                              [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-49                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─Sequential: 3-50                       [1, 2196, 256]            (recursive)\n",
       "├─TransformerEncoderBlock: 1-8                     [1, 64, 256]              (recursive)\n",
       "│    └─MultiheadSelfAttentionBlock: 2-19           [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-51                        [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-52                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-53                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─MultiheadAttention: 3-54               [1, 64, 256]              (recursive)\n",
       "│    └─MLPBlock: 2-20                              [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-55                        [1, 64, 256]              (recursive)\n",
       "│    │    └─Sequential: 3-56                       [1, 64, 256]              (recursive)\n",
       "├─TransformerEncoderBlock: 1-9                     [1, 2196, 256]            (recursive)\n",
       "│    └─MultiheadSelfAttentionBlock: 2-21           [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-57                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-58                        [1, 64, 256]              (recursive)\n",
       "│    │    └─LayerNorm: 3-59                        [1, 64, 256]              (recursive)\n",
       "│    │    └─MultiheadAttention: 3-60               [1, 2196, 256]            (recursive)\n",
       "│    └─MLPBlock: 2-22                              [1, 2196, 256]            (recursive)\n",
       "│    │    └─LayerNorm: 3-61                        [1, 2196, 256]            (recursive)\n",
       "│    │    └─Sequential: 3-62                       [1, 2196, 256]            (recursive)\n",
       "====================================================================================================\n",
       "Total params: 12,533,568\n",
       "Trainable params: 3,990,272\n",
       "Non-trainable params: 8,543,296\n",
       "Total mult-adds (G): 45.82\n",
       "====================================================================================================\n",
       "Input size (MB): 7.40\n",
       "Forward/backward pass size (MB): 60.33\n",
       "Params size (MB): 46.77\n",
       "Estimated Total Size (MB): 114.50\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=one_shot()\n",
    "summary(model=model,\n",
    "         input_size=[(1,3,128,128),(1,3,1000,600)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f88fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fad928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
